classdef LogitLinearModel < ClassificationModel
    properties(SetAccess = private)
        B1 = [];
        nonzeros = [];
    end
    
    methods(Static)
        function model = computeModel(A,y,w,options)
            
            assert(nnz(A)<1e8,['A has too many non-zeros: ', num2str(nnz(A))]);
            
            adj = false;
            if (adj)
                
                %now clean the labels
                %[U,S] = svd(full(double(A)),'econ');
                [U,S] = rand_svd(A, min(round(size(A,1)/2),2000), 2, 20);
                A_cut = U*S;
                
                size(A_cut)
                
                [x] = logit_regress([ones(size(A_cut,1),1), A_cut], y, 1.0e-7, w, 0);
                preds = glmval(x,A_cut,'logit');
                
                %get the right cutoff
                [~,~,~,~,optpoint] = perfcurve(y,preds,true,'Prior',[.95, .05]);
                [X,Y,T] = perfcurve(y,preds,true,'XVals',optpoint(1),'UseNearest','on');
                [X,Y,T]
                
                
                %perform readjustment of scores
                y = preds>=T;
                
                histogram(preds(y==0), 40);
                hold on;
                histogram(preds(y>0), 40);
                hold off;
                %pause;
            end
            
            %tic
            %options = statset('Display','iter','UseParallel',true);
            %[model,FitInfo] = lassoglm(A,y,'binomial','NumLambda',25,'CV',3,'Options',options,'LambdaRatio',1e-2,'Weights',w);
            
            lasso_alpha = options.lasso_alpha;
            assert(lasso_alpha>=0.0 & lasso_alpha<=1.0,'Invalid LassoAlpha value.');
                        
            options_glm.weights = w;
            options_glm.alpha = lasso_alpha;
            options_glm.thresh = 1.0e-5;
            %options_glm.maxit = 2e3;
            options_glm.maxit = 20;
            options_glm.ltype = 'modified.Newton';
            %options_glm.lambda_min = 5.0e-3;
            options_glm.lambda_min = 1.0e-4;
            
            %options.standardize = false;
            [model] = cvglmnet(double(A), y, 'binomial', glmnetSet(options_glm), [], 20, [], true, false, true);
            
            %cvglmnetPlot(model)
            
        end
    end
    
    methods
        
        function obj = LogitLinearModel(A,y,w,options)
            
            obj@ClassificationModel(A,y,options);

            if (isempty(w))
                w = ClassificationModel.getUniformWeights(y);
            end
            
            assert(length(w)==length(y),'Number of weights must match number of observations.');
            
            %now compute it
            model = LogitLinearModel.computeModel(A(:,obj.getActiveFeatures()),y,w,options);
            
            %indx = find(model.lambda==model.lambda_min);
            indx = find(model.lambda==model.lambda_1se);

            cnst = model.glmnet_fit.a0(indx);
            obj.B1 = [cnst;model.glmnet_fit.beta(:,indx)];
            
            %get the max value
            %maxval = max(abs(obj.B1(2:end)));

            obj.nonzeros = false(1, length(obj.B1)-1);
            obj.nonzeros(abs(obj.B1(2:end)) > 0) = true;
            
            w = obj.B1(2:end);
            maxval = max(abs(w));
            
            fprintf('Logit: Model contains %d non-zeros, and %d significant non-zeros, out of %d active features.\n', sum(obj.nonzeros), sum(abs(w)>maxval*1.0e-4), size(obj.getActiveFeatures(),2));
        end
        
        function z = margin(obj, A)
            assert(size(A,2)~=length(obj.sm),'Feature matrix not same size as original training matrix.');
            
            z = A*obj.B1(2:end)+obj.B1(1);
        end
        
        function y = predict(obj, A)
            predict@ClassificationModel(obj, A);
            
            y = glmval(obj.B1, double(A(:,obj.getActiveFeatures())),'logit');
        end
        
        function nnz = numberNonZeros(obj)
            if (islogical(obj.nonzeros))
                index = find(obj.nonzeros);
            else
                index = obj.nonzeros;            
            end
            
            nnz = length(index);
        end
                
        function offset = getModelOffset(obj)
            offset = obj.B1(1);
        end
        
        function weights = getModelWeights(obj)
            
            w =obj.B1(2:end);            

            features = obj.getActiveFeatures();
            
            weights = sparse(1,obj.getNumFeatures());           
            weights(features) = w;
        end
        
        function [] = explain(obj, A, names, column_labels, max_score, virus)
            
            if (nargin<6)
                virus = cell(size(A,1));
            end
            if (nargin<5)
                max_score = 0.0;
            end
                        
            p = obj.predict(A);

            A = A(:,obj.getActiveFeatures());
            column_labels = column_labels(obj.getActiveFeatures());
            
            weights = obj.B1(2:end)';
            max_weight = max(abs(weights));
            
            total_val = zeros(1, size(A,2));
            for iter=1:size(A,1)
                if (p(iter)>max_score)
                    fprintf('P=%.3f, %s, label=%s\n', p(iter), names{iter}, virus{iter});
                end
                
                %sort the values
                val = full(A(iter,:)).*weights;
                [~,I] = sort(abs(val),'descend');
                
                total_val = total_val+val;
                
                %figure out why
                iter2 = 1;
                if (p(iter)>max_score)
                    while (iter2<=length(I) && val(I(iter2))>max_weight*1.0e-4)
                        fprintf('\tW=%.4f, %s\n', val(I(iter2)), column_labels{I(iter2)});
                        iter2 = iter2+1;
                    end
                    
                    fprintf('\n');
                end
                
            end
            
            %now output the most common values
            [~,I] = sort(abs(total_val),'descend');
            max_val = abs(total_val(I(1)));
            iter = 1;

            fprintf('Most important observed features:\n');
            while abs(total_val(I(iter)))>max_val*1.0e-2
                fprintf('\tW=%.4f, %s\n', total_val(I(iter)), column_labels{I(iter)});
                iter = iter+1;
            end
            
        end
        
        function [] = makeJson(obj, column_labels, filename)
            
            weights = obj.getModelWeights();
            offset = obj.getModelOffset();
                        
            nzeros = find(abs(weights) > 1.0e-4*max(abs(weights)));

            [~,I] = sort(abs(weights(nzeros)),'descend');
            
            w_sorted = weights(nzeros(I));
            index_sorted = nzeros(I);
            
            struct.offset = offset;
            struct.weights = [];
            for iter=1:length(I)
                struct.weights(iter).weight = full(w_sorted(iter));
                %struct.weights(iter).index = index_sorted(iter);
                struct.weights(iter).name = strtrim(strsplit(column_labels{index_sorted(iter)},',\t'));
            end

            savejson('',struct,'FileName',filename);            
            
        end
        
    end % methods
end % classdef